{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# --Task 2 - Twitter post classifier (Using company labels)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this task, we need to predict the name of the company based on the inbound text from customers. For this problem, I am going with multi-class text classification using tensorflow.Keras "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Importing most of the libraries required for the code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "source": [
    "Loading the dataset using pandas and displaying the properties of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2811774 entries, 0 to 2811773\nData columns (total 7 columns):\n #   Column                   Dtype  \n---  ------                   -----  \n 0   tweet_id                 int64  \n 1   author_id                object \n 2   inbound                  bool   \n 3   created_at               object \n 4   text                     object \n 5   response_tweet_id        object \n 6   in_response_to_tweet_id  float64\ndtypes: bool(1), float64(1), int64(1), object(4)\nmemory usage: 131.4+ MB\nNone\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('twcs.csv')\n",
    "print(df1.info()) "
   ]
  },
  {
   "source": [
    "Splitting the dataset columns into individual lists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []; author_list = df1['author_id']\n",
    "text_list = []; text_list = df1['text']"
   ]
  },
  {
   "source": [
    "Knowing how many unique company names exist within the database. It is 91."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2811774 entries, 0 to 2811773\nData columns (total 7 columns):\n #   Column                   Dtype  \n---  ------                   -----  \n 0   tweet_id                 int64  \n 1   author_id                object \n 2   inbound                  bool   \n 3   created_at               object \n 4   text                     object \n 5   response_tweet_id        object \n 6   in_response_to_tweet_id  float64\ndtypes: bool(1), float64(1), int64(1), object(4)\nmemory usage: 131.4+ MB\n"
     ]
    }
   ],
   "source": [
    "authors = df1\n",
    "authors.info()\n",
    "authors = authors[authors.author_id.apply(lambda x: x.isalpha())]\n",
    "unique_authors_list = []\n",
    "unique_authors_list = authors['author_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['sprintcare' 'VerizonSupport' 'ChipotleTweets' 'AskPlayStation'\n 'marksandspencer' 'MicrosoftHelps' 'ATVIAssist' 'AdobeCare' 'AmazonHelp'\n 'XboxSupport' 'AirbnbHelp' 'nationalrailenq' 'AirAsiaSupport' 'Morrisons'\n 'NikeSupport' 'AskAmex' 'McDonalds' 'YahooCare' 'AskLyft' 'UPSHelp'\n 'Delta' 'AppleSupport' 'Tesco' 'SpotifyCares' 'comcastcares'\n 'AmericanAir' 'TMobileHelp' 'VirginTrains' 'SouthwestAir' 'AskeBay'\n 'GWRHelp' 'sainsburys' 'AskPayPal' 'HPSupport' 'ChaseSupport' 'CoxHelp'\n 'DropboxSupport' 'VirginAtlantic' 'AzureSupport' 'AlaskaAir'\n 'ArgosHelpers' 'AskTarget' 'GoDaddyHelp' 'CenturyLinkHelp' 'AskPapaJohns'\n 'askpanera' 'Walmart' 'USCellularCares' 'AsurionCares' 'GloCare'\n 'NeweggService' 'VirginAmerica' 'DunkinDonuts' 'TfL' 'asksalesforce'\n 'Kimpton' 'AskCiti' 'IHGService' 'LondonMidland' 'JetBlue' 'BoostCare'\n 'JackBox' 'AldiUK' 'HiltonHelp' 'GooglePlayMusic' 'OfficeSupport'\n 'DellCares' 'TwitterSupport' 'GreggsOfficial' 'ATT' 'TacoBellTeam'\n 'AskRBC' 'ArbysCares' 'NortonSupport' 'AskSeagate' 'sizehelpteam'\n 'SCsupport' 'MOO' 'AskDSC' 'AskVirginMoney' 'AskRobinhood' 'AWSSupport'\n 'VMUcare' 'mediatemplehelp' 'AskTigogh' 'PandoraSupport' 'askvisa'\n 'OPPOCareIN' 'PearsonSupport' 'CarlsJr' 'HotelTonightCX'] 91\n"
     ]
    }
   ],
   "source": [
    "print(unique_authors_list, len(unique_authors_list))"
   ]
  },
  {
   "source": [
    "Performing rearrangement of the data and printing out the total number of rows in the newly rearranged data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "957278\n"
     ]
    }
   ],
   "source": [
    "author_newlist, text_newlist = [], []\n",
    "\n",
    "for authorIndex in range(int(len(author_list))):\n",
    "    try:\n",
    "        if(author_list[authorIndex].isalpha()):\n",
    "            if(author_list[authorIndex+1].isnumeric()):\n",
    "                text_newlist.append(text_list[authorIndex+1])\n",
    "                author_newlist.append(author_list[authorIndex])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "print(len(text_newlist))"
   ]
  },
  {
   "source": [
    "Checking the newly formed data for garbage useless values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sprintcare : @sprintcare and how do you propose we do that\nsprintcare : @sprintcare I did.\nsprintcare : @sprintcare is the worst customer service\nsprintcare : @sprintcare You gonna magically change your connectivity for me and my whole family ? ü§• üíØ\nsprintcare : @sprintcare Since I signed up with you....Since day 1\nsprintcare : @115714 y‚Äôall lie about your ‚Äúgreat‚Äù connection. 5 bars LTE, still won‚Äôt load something. Smh.\nsprintcare : @115714 whenever I contact customer support, they tell me I have shortcode enabled on my account, but I have never in the 4 years I've tried https://t.co/0G98RtNxPK\nVerizonSupport : @VerizonSupport I finally got someone that helped me, thanks!\nVerizonSupport : somebody from @VerizonSupport please help meeeeee üò©üò©üò©üò© I'm having the worst luck with your customer service\nVerizonSupport : @VerizonSupport My friend is without internet we need to play videogames together please our skills diminish every moment without internetz\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):   \n",
    "    print(author_newlist[i], \":\", text_newlist[i])"
   ]
  },
  {
   "source": [
    "Created a dataframe for newly formed data and verified it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           author                                               text\n0      sprintcare      @sprintcare and how do you propose we do that\n1      sprintcare                                 @sprintcare I did.\n2      sprintcare          @sprintcare is the worst customer service\n3      sprintcare  @sprintcare You gonna magically change your co...\n4      sprintcare  @sprintcare Since I signed up with you....Sinc...\n5      sprintcare  @115714 y‚Äôall lie about your ‚Äúgreat‚Äù connectio...\n6      sprintcare  @115714 whenever I contact customer support, t...\n7  VerizonSupport  @VerizonSupport I finally got someone that hel...\n8  VerizonSupport  somebody from @VerizonSupport please help meee...\n9  VerizonSupport  @VerizonSupport My friend is without internet ...\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "  \"author\" : author_newlist,\n",
    "  \"text\": text_newlist\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head(10)) "
   ]
  },
  {
   "source": [
    "Definitions and modules for cleaning the texts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Likhi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_emoji(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "source": [
    "Performing the cleaning and removing useless information from data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:6: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    }
   ],
   "source": [
    "df['text'] = df.text.map(lambda x: remove_URL(x)) #remove URL elements\n",
    "df['text'] = df.text.map(lambda x: remove_html(x)) #remove htmlelements\n",
    "df['text'] = df.text.map(lambda x: remove_emoji(x)) #remove emojis\n",
    "df['text'] = df.text.map(lambda x: remove_punct(x)) #remove punctuations\n",
    "df['text'] = df.text.map(lambda x: remove_stopwords(x)) #remove stopwords\n",
    "df['text'] = df['text'].str.replace('\\d+', '') #remove numbers from text strings"
   ]
  },
  {
   "source": [
    "Due to my hardware issues, dividing the data into six bins - equal portions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 6\n",
    "frames = np.array_split(df, bins)"
   ]
  },
  {
   "source": [
    "Verifying."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Checking all the bins for total number of labels and rows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bin  0 :  number of labels/classes ->  159547 and Number of rows ->  91\nBin  1 :  number of labels/classes ->  159547 and Number of rows ->  91\nBin  2 :  number of labels/classes ->  159547 and Number of rows ->  91\nBin  3 :  number of labels/classes ->  159547 and Number of rows ->  91\nBin  4 :  number of labels/classes ->  159547 and Number of rows ->  91\nBin  5 :  number of labels/classes ->  159547 and Number of rows ->  91\n"
     ]
    }
   ],
   "source": [
    "bin_index = 0\n",
    "for i in range(int(len(frames))):\n",
    "    print(\"Bin \", i, \": \", \"number of labels/classes -> \", len(frames[1][\"author\"]), \"and Number of rows -> \", len(frames[i][\"author\"].unique()))"
   ]
  },
  {
   "source": [
    "Setting up a counter for counting the total number of words in the selected bin and printing them out."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "78027\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#counts unique words from all text column\n",
    "def counter_word(text):\n",
    "    count = Counter()\n",
    "    for i in text.values:\n",
    "        for word in i.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "max_words = len(counter_word(frames[bin_index][\"text\"]))\n",
    "print(max_words)"
   ]
  },
  {
   "source": [
    "Checking and setting the sizes of test and train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train size: 111682\nTest size: 47865\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(frames[bin_index]) * .7)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(frames[bin_index]) - train_size))"
   ]
  },
  {
   "source": [
    "Spliting the reformed data into test and train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_posts = frames[bin_index]['text'][:train_size]\n",
    "train_tags = frames[bin_index]['author'][:train_size]\n",
    "\n",
    "test_posts = frames[bin_index]['text'][train_size:]\n",
    "test_tags = frames[bin_index]['author'][train_size:]"
   ]
  },
  {
   "source": [
    "Creating a tokenizer and Tokenizing the text column of test and train."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)"
   ]
  },
  {
   "source": [
    "Encoding labels/Classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "source": [
    "Verifying."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x_train shape: (111682, 78027)\nx_test shape: (47865, 78027)\ny_train shape: (111682, 91)\ny_test shape: (47865, 91)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "source": [
    "Setting up batch size (random choice) and epochs (model is overfitting so I chose low epoch number)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "source": [
    "My model design and compilation of the design."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "Fitting my model using the parametres and variables given above. Highest accuracy that I got is 95.58%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2\n",
      "3142/3142 [==============================] - 388s 96ms/step - loss: 1.0953 - accuracy: 0.7885 - val_loss: 0.2913 - val_accuracy: 0.9251\n",
      "Epoch 2/2\n",
      "3142/3142 [==============================] - 122s 39ms/step - loss: 0.1712 - accuracy: 0.9558 - val_loss: 0.2961 - val_accuracy: 0.9241\n"
     ]
    }
   ],
   "source": [
    "Tweet_model = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "source": [
    "Evaluating my model: Accuracy is 92.046%"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1496/1496 [==============================] - 39s 23ms/step - loss: 0.3224 - accuracy: 0.9205\n",
      "Test accuracy: 0.9204638004302979\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "source": [
    "Visual result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "amazonhelp metal credit card impacting qi charging ...\n",
      "Actual label:AmazonHelp\n",
      "Predicted label: AmazonHelp\n",
      "\n",
      " hola received item missing part contact seller th ...\n",
      "Actual label:AmazonHelp\n",
      "Predicted label: AskeBay\n",
      "\n",
      " policy price changes purchase ...\n",
      "Actual label:AmazonHelp\n",
      "Predicted label: AmazonHelp\n",
      "\n",
      "applesupport contact  know specifics ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n",
      " solid customer service  ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AmazonHelp\n",
      "\n",
      " applesupport yes update twitter apps lagging also ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n",
      "phone crashes every mins banging love new update  ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n",
      "applesupport won‚Äôt videos stream devices onto appl ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n",
      "applesupport  international dialing code brazil io ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n",
      "applesupport home screen look locked time datelate ...\n",
      "Actual label:AppleSupport\n",
      "Predicted label: AppleSupport\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_labels = encoder.classes_ \n",
    "\n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction)]\n",
    "    print(test_posts.iloc[i][:50], \"...\")\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label + \"\\n\")"
   ]
  },
  {
   "source": [
    "Saving the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('saved_model/my_model2')"
   ]
  },
  {
   "source": [
    "Loading the model (Commented out since it has no purpose)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = tf.keras.models.load_model('saved_model/my_model2')"
   ]
  },
  {
   "source": [
    "model summary (Commented out since it has no purpose)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model.summary()"
   ]
  }
 ]
}